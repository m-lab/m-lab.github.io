---
layout: blog
title: "Internet performance talk at IGF 2018"
author: "Simone Basso"
date: 2019-02-12
breadcrumb: blog
categories:
  - igf
  - event
---

In this blog post I’ll share the slides I presented in a session at the 2018 Internet Governance Forum (IGF) titled “[Net Neutrality: Measuring Discriminatory Practices](https://www.intgovforum.org/multilingual/content/igf-2018-dc-net-neutrality-measuring-discriminatory-practices-dcnn){:target="_blank}”, along with some extra comments that occurred to me after delivering the talk, when preparing this follow-up blog post.<!--more-->

In my first slide, I recapped [OONI](https://ooni.torproject.org/){:target="_blank"}’s experience with measuring performance of an [emulated DASH flow](https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP){:target="_blank"} in collaboration with [Fight for the Future](https://www.fightforthefuture.org/){:target="_blank"}. What we did was basically reimplement in [Measurement Kit](https://github.com/measurement-kit/measurement-kit){:target="_blank"} (OONI’s measurement library) [Neubot’s DASH test](https://github.com/ooni/spec/blob/696dcbf76e89ae32f53e7f552a524bed41ee0d05/nettests/ts-021-dash.md){:target="_blank"} and run experiments.

![Slide 1 image]({{ site.baseurl }}/images/blog/IGF2018-1.png)

After mentioning that, I went on to describe what I learned from that experience, and from reasoning more in general about measuring network-neutrality-related metrics. I started by providing an historical perspective, to then give an overview of the current landscape.

My second slide, in fact, describes the measurement scenario around the end of the previous decade, where the emergence of Measurement Lab allowed to write internet measurement experiments that users could run from their computers towards [measurement servers provided by Measurement Lab](http://mlab-ns.appspot.com/admin/map/ipv4/all){:target="_blank"}. I went on to explain how this measurement infrastructure was excellent to tackle the most pressing measurement issues at the time (i.e. t[he performance with which users could access internet content]({{ site.baseurl }}/publications/understanding-broadband-speed-measurements.pdf) and [the selective throttling of specific peer-to-peer protocols](https://arstechnica.com/uncategorized/2007/11/eff-study-reveals-evidence-of-comcasts-bittorrent-interference/){:target="_blank"} and, chiefly among them, BitTorrent).

![Slide 2 image]({{ site.baseurl }}/images/blog/IGF2018-2.png)

Regarding speed measurements, Measurement Lab servers are traditionally located in data centers and [Internet eXchange Points](https://en.wikipedia.org/wiki/Internet_exchange_point){:target="_blank"} (IXPs) that are close enough to internet content. So, tests like [NDT](https://github.com/ndt-project/ndt){:target="_blank"} and [Neubot](https://github.com/neubot/neubot){:target="_blank"} (the blue line in figure) could provide a reasonable proxy of the performance of “using the internet” (the yellow line in figure).

While discussing this matter, I also pointed out that measurements in many cases do not take into account the fact that there is a wireless link on path (a WiFi in the picture), and that this is an additional source of complexity because of interference and other effects. (Incidentally, this problem is even more urgent today, [with the surge of 2-3-4G usage](https://www.statista.com/topics/779/mobile-internet/){:target="_blank"}.

Regarding measuring protocol throttling, [Glasnost](http://broadband.mpi-sws.org/transparency/results/10_nsdi_glasnost.pdf){:target="_blank"} and other experiments relied on the fact that ISPs were discriminating against any BitTorrent-like flow, regardless of its destination. So, a server located in a well connected datacenter was functionally equivalent to a BitTorrent peer in another ISP (see the red line in figure). What’s more, since Measurement Lab servers were generally much better provisioned than random BitTorrent peers, it was quite sensible to attribute performance issues to either user’s issues or ISP’s interference. And user’s issues (e.g. WiFi issues) could be singled out by running non BitTorrent tests towards the same server, in order to establish a performance baseline for the path.

The third slide illustrates the current situation (of course, IMO). Here, I emphasize the importance of [Content Delivery Networks](https://en.wikipedia.org/wiki/Content_delivery_network){:target="_blank"} (CDNs) because interconnection is increasingly becoming one of the most pressing network neutrality issues. Since 2008, [video has been more and more widely used](https://www.webmarketingpros.com/internet-video-to-account-for-80-of-global-traffic-by-2019/){:target="_blank"}. So, content providers and ISPs have used direct interconnections much more frequently, to reduce latency and to better control quality (i.e. losses).

![Slide 3 image]({{ site.baseurl }}/images/blog/IGF2018-3.png)

In figure, the violet line is a direct interconnection between an ISP and a CDN. We have seen several cases where the ISP and the CDN had a peering war, where one party forced the other to take a longer and likely more noisy path (the orange path in figure). One of the first such cases to go mainstream was [the tussle between Comcast and Level 3](https://arstechnica.com/tech-policy/2010/12/comcastlevel3/){:target="_blank"}. In that case, and in many other cases, the longer route obviously also lowered the quality of the streaming, because of higher round trip times, queueing delays, and/or packet losses.

So, understanding whether there is “throttling” of specific video streams has increasingly become a hot policy issue. However, the change in the infrastructure that accompanied the expansion of already large Content Delivery Networks has made the job of researchers operating with public measurement tools more complex. In many cases it is either not possible or too expensive to deploy measurement servers near the data.

One solution to this problem is to stream real content and measure its performance. This is what Sam Knows does, [according to its own documentation](https://support.samknows.one/hc/en-gb/articles/115003164329-How-do-you-test-video-sites-like-YouTube-and-Netflix-){:target="_blank"}, targeting YouTube, Netflix, BBC iPlayer and possibly other video streaming services. In addition to that, they also perform active measurements, presumably with Measurement Lab servers, that emulate video streaming (like the above mentioned Neubot DASH test does). Albeit this methodology is not open source, it should in principle be able to detect peering issues, _because it measures the actual path_.

Another leading tool in this space is [WeHe](https://dd.meddle.mobi/){:target="_blank"}. This app, [supported by the French regulator ARCEP](https://www.clubic.com/internet/actualite-847297-neutralite-net-arcep-wehe-appli-detecte-bridages-flux.html){:target="_blank"}, basically “replays” the packets captured during a previous streaming session (e.g. with Netflix or YouTube) using as destination server a server controlled by the researchers rather than YouTube. (In the future, WeHe should use as destination Measurement Lab servers, so you can consider the blue flow in figure as a WeHe replay flow.) This has proven to be a good approach for detecting when ISPs throttle traffic directed to a specific service based on some feature of the traffic (e.g. [the SNI in the TLS HELLO packet](https://en.wikipedia.org/wiki/Server_Name_Indication){:target="_blank"}). In many regards, WeHe’s methodology is similar to Glasnost’s methodology and it is probably the best way to use existing platforms like Measurement Lab to measure throttling of video services. However, I still fail to see how this approach could detect a peering issue like the one in figure, because the network test and video streaming may in principle use two distinct network paths.

I concluded my talk mentioning that one may be tempted to conclude that having measurement servers inside CDNs is the definitive way to spot interconnection issues. Since this seems to be complex to do, I said I believe that a mixture of techniques for detecting signature based throttling (like WeHe does), interconnection issues (by measuring the service directly like SamKnows does), and baselines (like SamKnows and Nebot DASH do) is probably a reasonable second best solution to address the video throttling policy question.
